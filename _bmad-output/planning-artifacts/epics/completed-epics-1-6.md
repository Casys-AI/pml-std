# Casys PML - Completed Epics Archive (1-6)

**Archived:** 2025-12-05 **Reason:** Keeping epics.md lean per BMAD methodology - active epics only

---

## Epic 1: Project Foundation & Context Optimization Engine

**Status:** ‚úÖ DONE (2025-11-05) **Retrospective:** Completed

**Expanded Goal (2-3 sentences):**

√âtablir l'infrastructure projet Deno avec CI/CD, impl√©menter le syst√®me de vector search s√©mantique
via PGlite + pgvector, et cr√©er le moteur de context optimization qui r√©duit la consommation de
contexte de 30-50% √† <5%. Ce premier epic livre un syst√®me fonctionnel permettant le chargement
on-demand des tool schemas MCP, validant la proposition de valeur principale d'Casys PML et
√©tablissant les foundations pour la parall√©lisation (Epic 2).

**Value Delivery:**

√Ä la fin de cet epic, un d√©veloppeur peut installer Casys PML, migrer sa configuration MCP, et
observer imm√©diatement une r√©duction du contexte √† <5%, r√©cup√©rant 90% de sa fen√™tre
conversationnelle pour usage utile.

---

### Story Breakdown - Epic 1

**Story 1.1: Project Setup & Repository Structure**

As a developer, I want a clean Deno project structure with CI/CD configured, So that I can start
development with proper tooling and automation in place.

**Acceptance Criteria:**

1. Repository initialis√© avec structure Deno standard (src/, tests/, docs/)
2. GitHub Actions CI configur√© (lint, typecheck, tests)
3. deno.json configur√© avec tasks scripts (test, lint, fmt, dev)
4. README.md avec badges CI et quick start guide
5. .gitignore appropri√© pour Deno projects
6. License MIT et CODE_OF_CONDUCT.md

**Prerequisites:** None

---

**Story 1.2: PGlite Database Foundation with pgvector**

As a developer, I want a PGlite database with pgvector extension configured, So that I can store
embeddings vectoriels et perform semantic search efficiently.

**Acceptance Criteria:**

1. PGlite database initialization dans `~/.pml/.pml.db`
2. pgvector extension loaded et operational
3. Database schema cr√©√© avec tables:
   - `tool_embedding` (tool_id, embedding vector(1024), metadata)
   - `tool_schema` (tool_id, schema_json, server_id, cached_at)
   - `config` (key, value pour metadata)
4. Vector index HNSW cr√©√© sur tool_embedding.embedding avec pgvector
5. Basic CRUD operations test√©s (insert, query, update, delete)
6. Database migration system en place pour schema evolution future

**Prerequisites:** Story 1.1 (project setup)

---

**Story 1.3: MCP Server Discovery & Schema Extraction**

As a power user with 15+ MCP servers, I want Casys PML to automatically discover my MCP servers and
extract their tool schemas, So that I don't have to manually configure each server.

**Acceptance Criteria:**

1. MCP server discovery via stdio et SSE protocols
2. Connection √©tablie avec chaque discovered server
3. Tool schemas extracted via MCP protocol `list_tools` call
4. Schemas parsed et validated (input/output schemas, descriptions)
5. Schemas stock√©s dans PGlite `tool_schema` table
6. Error handling pour servers unreachable ou invalid schemas
7. Console output affiche nombre de servers discovered et tools extracted
8. Support au minimum 15 MCP servers simultan√©ment

**Prerequisites:** Story 1.2 (database foundation)

---

**Story 1.4: Embeddings Generation with BGE-Large-EN-v1.5**

As a developer, I want tool schemas to be converted into vector embeddings using BGE-Large-EN-v1.5
locally, So that I can perform semantic search without relying on external APIs.

**Acceptance Criteria:**

1. BGE-Large-EN-v1.5 model downloaded et loaded (via @xenova/transformers)
2. Tool schemas (name + description + parameters) concaten√©s en text input
3. Embeddings (1024-dim) g√©n√©r√©s pour chaque tool
4. Embeddings stock√©s dans `tool_embeddings` table avec metadata
5. Progress bar affich√©e durant g√©n√©ration (peut prendre ~60s pour 100+ tools)
6. Embeddings cach√©s (pas de r√©g√©n√©ration si schema unchanged)
7. Total generation time <2 minutes pour 200 tools

**Prerequisites:** Story 1.3 (schema extraction)

---

**Story 1.5: Semantic Vector Search Implementation**

As a developer, I want to search for relevant tools using natural language queries, So that I can
find the right tools without knowing their exact names.

**Acceptance Criteria:**

1. Query embedding g√©n√©ration (m√™me mod√®le BGE-Large-EN-v1.5)
2. Cosine similarity search sur vector index (<100ms query time P95)
3. API: `searchTools(query: string, topK: number)` ‚Üí tool_ids + scores
4. Top-k results returned sorted par relevance score (default k=5)
5. Configurable similarity threshold (default 0.7)
6. Unit tests validant accuracy avec sample queries
7. Benchmark test confirmant P95 <100ms pour 1000+ vectors

**Prerequisites:** Story 1.4 (embeddings generation)

---

**Story 1.6: On-Demand Schema Loading & Context Optimization**

As a Claude Code user, I want Casys PML to load only relevant tool schemas based on my query, So
that my context window is not saturated by unused tool schemas.

**Acceptance Criteria:**

1. Integration semantic search avec schema loading
2. Workflow: query ‚Üí vector search ‚Üí retrieve top-k tools ‚Üí load schemas
3. Schemas retourn√©s uniquement pour matched tools (pas all-at-once)
4. Context usage measurement et logging (<5% target)
5. Comparison metric affich√©: before (30-50%) vs after (<5%)
6. Cache hit pour frequently used tools (√©vite reloading)
7. Performance: Total query-to-schema latency <200ms P95

**Prerequisites:** Story 1.5 (vector search)

---

**Story 1.7: Migration Tool (`pml init`)**

As a power user with existing MCP configuration, I want to migrate my mcp.json configuration to
Casys PML automatically, So that I don't have to manually reconfigure everything.

**Acceptance Criteria:**

1. CLI command `pml init` implemented
2. Detection automatique du claude_desktop_config.json path (OS-specific)
3. Parsing du mcp.json existant et extraction des MCP servers
4. Generation de `~/.pml/config.yaml` avec servers migr√©s
5. Embeddings generation triggered automatiquement post-migration
6. Console output avec instructions pour √©diter mcp.json
7. Template affich√© pour nouvelle config mcp.json (juste pml gateway)
8. Rollback capability si erreur durant migration
9. Dry-run mode (`--dry-run`) pour preview changes

**Prerequisites:** Story 1.6 (context optimization functional)

---

**Story 1.8: Basic Logging & Telemetry Backend**

As a developer, I want structured logging et m√©triques telemetry opt-in, So that I can debug issues
et measure success metrics (context usage, latency).

**Acceptance Criteria:**

1. Structured logging avec std/log (Deno standard library)
2. Log levels: error, warn, info, debug
3. Log output: console + file (`~/.pml/logs/pml.log`)
4. Telemetry table dans PGlite: `metrics` (timestamp, metric_name, value)
5. Metrics tracked: context_usage_pct, query_latency_ms, tools_loaded_count
6. Opt-in consent prompt au premier launch (telemetry disabled by default)
7. CLI flag `--telemetry` pour enable/disable
8. Privacy: aucune data sensitive (queries, schemas) ne quitte local machine

**Prerequisites:** Story 1.7 (migration tool ready)

---

## Epic 2: DAG Execution & Production Readiness

**Status:** ‚úÖ DONE **Retrospective:** Completed

**Expanded Goal (2-3 sentences):**

Impl√©menter le syst√®me de DAG execution pour parall√©lisation intelligente des workflows multi-tools,
int√©grer Casys PML comme MCP gateway avec Claude Code, et hardening production avec health checks,
error handling robuste, et tests end-to-end. Ce second epic livre un syst√®me production-ready
capable de r√©duire la latence des workflows de 5x √† 1x via parall√©lisation, compl√©tant ainsi la
double value proposition d'Casys PML (context + speed).

**Architecture Clarification: GraphRAG vs DAG:**

Il est crucial de comprendre la distinction entre deux composants architecturaux compl√©mentaires :

- **GraphRAG (Epic 1)** = Base de connaissances globale
  - Stocke TOUS les tools de TOUS les MCP servers (687 tools)
  - Contient l'historique des workflows ex√©cut√©s (succ√®s/√©checs, patterns)
  - Maintient les relations entre tools (ex: "filesystem:read" suivi de "json:parse" dans 85% des
    cas)
  - **Scope:** Global, toutes les possibilit√©s

- **DAG (Epic 2)** = Instance de workflow sp√©cifique
  - Un workflow concret pour UNE t√¢che pr√©cise
  - Contient uniquement les 3-5 tools pertinents pour cette requ√™te
  - D√©finit explicitement les d√©pendances (task B d√©pend de task A)
  - **Scope:** Local, single execution

**Comment ils permettent le Speculative Execution:**

```
GraphRAG (Epic 1) ‚Üí Apprend les patterns historiques
        ‚Üì
DAG Suggester ‚Üí Pr√©dit quel DAG construire bas√© sur l'intent
        ‚Üì
DAG (Epic 2) ‚Üí Structure concr√®te √† ex√©cuter
        ‚Üì
Execution Sp√©culative ‚Üí Lance le DAG pr√©dit AVANT que l'agent demande
        ‚Üì
R√©sultats cach√©s ‚Üí Agent obtient r√©ponse instantan√©e
```

Sans GraphRAG (la connaissance), impossible de pr√©dire quel DAG construire. Sans DAG (la structure),
impossible d'ex√©cuter en parall√®le ou sp√©culativement.

Le **Speculative Execution** n'est possible que gr√¢ce au **graph de d√©pendances** qui encode les
patterns appris dans GraphRAG et permet la pr√©diction de workflows complets.

**Value Delivery:**

√Ä la fin de cet epic, un d√©veloppeur peut ex√©cuter des workflows cross-MCP complexes avec
parall√©lisation automatique, observant des gains de performance 3-5x sur workflows typiques, le tout
via une gateway stable et fiable int√©gr√©e √† Claude Code.

---

### Story Breakdown - Epic 2

**Story 2.1: Dependency Graph Construction (DAG Builder)**

As a developer, I want Casys PML to automatically construct a dependency graph from tool
input/output schemas, So that independent tools can be identified for parallel execution.

**Acceptance Criteria:**

1. DAG builder module cr√©√© (`src/dag/builder.ts`)
2. Parsing des tool input/output schemas (JSON Schema format)
3. Dependency detection: tool B depends on tool A si output_A matches input_B
4. DAG representation: nodes (tools) + edges (dependencies)
5. Topological sort implementation (custom, zero external dependency)
6. Detection de cycles (DAG invalide) avec error reporting
7. Unit tests avec sample workflows (sequential, parallel, mixed)
8. API: `buildDAG(tools: Tool[])` ‚Üí DAG graph object

**Prerequisites:** Epic 1 compl√©t√© (context optimization functional)

---

**Story 2.2: Parallel Execution Engine**

As a power user, I want workflows avec independent tools to execute in parallel, So that I save time
instead of waiting for sequential execution.

**Acceptance Criteria:**

1. Parallel executor module cr√©√© (`src/dag/executor.ts`)
2. DAG traversal avec identification des nodes ex√©cutables en parall√®le
3. Promise.all utilis√© pour parallel execution de independent branches
4. Sequential execution pour dependent tools (respect topological order)
5. Partial success handling: continue execution m√™me si un tool fail
6. Results aggregation: successes + errors retourn√©s avec codes
7. Performance measurement: latency avant/apr√®s parall√©lisation
8. Target: P95 latency <3 secondes pour workflow 5-tools
9. Benchmarks tests validant 3-5x speedup sur workflows parall√©lisables

**Prerequisites:** Story 2.1 (DAG builder)

---

**Story 2.3: SSE Streaming pour Progressive Results**

As a user waiting for workflow results, I want to see results streamed progressively as they
complete, So that I get feedback immediately instead of waiting for all tools to finish.

**Acceptance Criteria:**

1. SSE (Server-Sent Events) implementation pour streaming
2. Event types d√©finis: `task_start`, `task_complete`, `execution_complete`, `error`
3. Results stream√©s d√®s disponibilit√© (pas de wait-all-then-return)
4. Event payload: tool_id, status, result, timestamp
5. Client-side handling simul√© dans tests
6. Graceful degradation si SSE unavailable (fallback to batch response)
7. Max event buffer size pour √©viter memory leaks

**Prerequisites:** Story 2.2 (parallel executor)

---

**Story 2.4: MCP Gateway Integration avec Claude Code**

As a Claude Code user, I want Casys PML to act as a transparent MCP gateway, So that Claude can
interact with all my MCP servers via a single entry point.

**Acceptance Criteria:**

1. MCP protocol server implementation (stdio mode primary)
2. Casys PML expose MCP server interface compatible avec Claude Code
3. Requests de Claude intercept√©s par gateway
4. Vector search ‚Üí load schemas ‚Üí execute tools ‚Üí return results
5. Transparent proxying: Claude voit Casys PML comme un seul MCP server
6. Support `list_tools`, `call_tool`, `get_prompt` methods (MCP spec)
7. Error handling: MCP-compliant error responses
8. Integration test avec mock Claude client

**Prerequisites:** Story 2.3 (SSE streaming ready)

---

**Story 2.5: Health Checks & MCP Server Monitoring**

As a developer, I want Casys PML to monitor MCP server health et report issues, So that I know which
servers are down or misconfigured.

**Acceptance Criteria:**

1. Health check implementation au startup (ping chaque MCP server)
2. Periodic health checks (every 5 minutes) durant runtime
3. Health status tracking: healthy, degraded, down
4. Console warnings pour servers unavailable
5. Automatic retry logic (3 attempts) avant marking server down
6. Health status API: `pml status` CLI command
7. Logs structured avec server_id, status, last_check timestamp

**Prerequisites:** Story 2.4 (gateway integration)

---

**Story 2.6: Error Handling & Resilience**

As a developer, I want robust error handling throughout Casys PML, So that the system degrades
gracefully instead of crashing.

**Acceptance Criteria:**

1. Try-catch wrappers autour de all async operations
2. Error types d√©finis: MCPServerError, VectorSearchError, DAGExecutionError
3. User-friendly error messages avec suggestions de resolution
4. Rollback capability pour failed migrations
5. Partial workflow success (return succ√®s m√™me si some tools fail)
6. Timeout handling (default 30s per tool execution)
7. Rate limiting pour prevent MCP server overload
8. Error logs persist√©s pour post-mortem analysis

**Prerequisites:** Story 2.5 (health checks)

---

**Story 2.7: End-to-End Tests & Production Hardening**

As a developer shipping production software, I want comprehensive E2E tests et production hardening,
So that Casys PML is reliable et users don't experience bugs.

**Acceptance Criteria:**

1. E2E test suite cr√©√© avec Deno.test
2. Test scenarios: migration, vector search, DAG execution, gateway proxying
3. Mock MCP servers pour testing (fixtures)
4. Integration tests avec real BGE-Large model
5. Performance regression tests (benchmark suite)
6. Memory leak detection tests (long-running daemon)
7. CI configuration updated pour run E2E tests
8. Code coverage report >80% (unit + integration)
9. Load testing: 15+ MCP servers, 100+ tools
10. Documentation: README updated avec installation, usage, troubleshooting

**Prerequisites:** Story 2.6 (error handling)

---

## Epic 2.5: Adaptive DAG Feedback Loops (Foundation)

**Status:** ‚úÖ DONE (2025-12-01, 4/4 stories) **Retrospective:** Completed

**Expanded Goal (2-3 sentences):**

√âtablir la fondation pour workflows adaptatifs avec feedback loops Agent-in-the-Loop (AIL) et
Human-in-the-Loop (HIL), pr√©parant l'int√©gration avec Epic 3 (Sandbox). Impl√©menter l'architecture
3-Loop Learning (Phase 1 - Foundation) avec event stream observable, checkpoint/resume, et DAG
replanning dynamique. Ce pivot architectural d√©bloque le contr√¥le runtime essentiel pour les
op√©rations critiques (HIL approval code sandbox Epic 3) et workflows adaptatifs d√©couvrant
progressivement leurs besoins.

**Architecture 3-Loop Learning (Phase 1 - Foundation):**

**Loop 1 (Execution - Real-time):**

- Event stream observable pour monitoring en temps r√©el
- Command queue pour contr√¥le dynamique (agent + humain)
- State management avec checkpoints et resume
- **Fr√©quence:** Milliseconds (pendant l'ex√©cution)

**Loop 2 (Adaptation - Runtime):**

- Agent-in-the-Loop (AIL): D√©cisions autonomes pendant l'ex√©cution
- Human-in-the-Loop (HIL): Validation humaine pour op√©rations critiques
- DAG re-planning dynamique via GraphRAG queries
- **Fr√©quence:** Seconds √† minutes (entre layers)

**Loop 3 (Meta-Learning - Basic):**

- GraphRAG updates from execution patterns (co-occurrence, preferences)
- Learning baseline pour futures optimisations
- **Fr√©quence:** Per-workflow

**Value Delivery:**

√Ä la fin de cet epic, Casys PML peut adapter ses workflows en temps r√©el bas√© sur les d√©couvertes
runtime, demander validation humaine pour op√©rations critiques, et apprendre des patterns
d'ex√©cution pour am√©liorer futures suggestions. Foundation critique pour Epic 3 (HIL code sandbox
approval) et Epic 3.5 (speculation with rollback).

---

### Story Breakdown - Epic 2.5

**Story 2.5-1: Event Stream, Command Queue & State Management**

As a developer building adaptive workflows, I want real-time event streaming and dynamic control
capabilities, So that I can observe execution progress and inject commands during runtime.

**Acceptance Criteria:**

1. `ControlledExecutor` extends `ParallelExecutor` (Epic 2) avec event stream
2. Event types d√©finis: `workflow_started`, `task_started`, `task_completed`, `workflow_completed`,
   `error`, `awaiting_input`
3. EventEmitter implementation (Node.js-style events)
4. Command queue: `pause`, `resume`, `cancel`, `replan`, `inject_task`
5. State management: workflow state =
   `{ status, current_tasks, completed_tasks, pending_tasks, checkpoints }`
6. State serialization/deserialization (JSON-compatible)
7. Thread-safe command injection (async queue)
8. Unit tests: event emission, command processing, state transitions
9. Integration test: Execute workflow ‚Üí inject pause command ‚Üí verify workflow pauses

**Prerequisites:** Epic 2 completed (ParallelExecutor functional)

---

**Story 2.5-2: Checkpoint & Resume Infrastructure**

As a user with long-running workflows, I want workflows to be resumable after interruptions, So that
I don't lose progress if something fails or I need to stop.

**Acceptance Criteria:**

1. Checkpoint syst√®me impl√©ment√© (`src/dag/checkpoint.ts`)
2. Checkpoints stock√©s dans PGlite table: `workflow_checkpoints` (workflow_id, state_json,
   timestamp)
3. Checkpoint automatique: apr√®s chaque task completed, before each critical operation
4. Resume API: `resumeWorkflow(workflow_id)` ‚Üí reconstruit state et continue
5. Partial result preservation: completed tasks results cached
6. Task idempotency verification: detect if task already completed before retry
7. Checkpoint cleanup: auto-delete checkpoints >7 days old
8. CLI command: `pml resume <workflow_id>`
9. Error handling: corrupt checkpoint ‚Üí fallback to nearest valid checkpoint
10. Integration test: Workflow fails mid-execution ‚Üí resume ‚Üí completes successfully

**Prerequisites:** Story 2.5-1 (state management)

---

**Story 2.5-3: AIL/HIL Integration & DAG Replanning**

As an AI agent executing complex workflows, I want to make autonomous decisions (AIL) and request
human validation (HIL) when needed, So that workflows can adapt based on discoveries and critical
operations get human oversight.

**Acceptance Criteria:**

1. AIL (Agent-in-the-Loop) implementation:
   - Decision points d√©finis dans DAG: `{ type: 'ail_decision', prompt: string, options: [...] }`
   - Agent query mechanism via single conversation thread (no context filtering)
   - Multi-turn conversation support for complex decisions
   - Decision logging dans PGlite: `ail_decisions` (workflow_id, decision_point, chosen_option,
     rationale)

2. HIL (Human-in-the-Loop) implementation:
   - Approval gates pour critical operations:
     `{ type: 'hil_approval', operation: string, risk_level: 'low'|'medium'|'high' }`
   - User prompt via CLI or API: "Approve code execution? [y/n]"
   - Timeout handling: auto-reject after 5 minutes (configurable)
   - Approval history logging

3. DAG Replanning:
   - `DAGSuggester.replanDAG(current_state, new_intent)` method
   - GraphRAG query pour find alternative paths
   - Merge new DAG avec existing execution state
   - Preserve completed tasks, replace pending tasks
   - Validation: no cycles introduced, dependencies preserved

4. Integration with ControlledExecutor:
   - Pause workflow at decision/approval points
   - Emit `awaiting_input` event
   - Resume after decision/approval received

5. Tests:
   - AIL test: Workflow encounters decision point ‚Üí agent chooses option ‚Üí workflow continues
   - HIL test: Critical operation ‚Üí human approves ‚Üí execution proceeds
   - Replanning test: Workflow discovers new requirement ‚Üí replan ‚Üí new tasks added
   - Multi-turn test: Agent asks follow-up questions before decision

**Prerequisites:** Story 2.5-2 (checkpoint/resume)

---

**Story 2.5-4: Command Infrastructure Hardening** _(Scope Reduced per ADR-018)_

> **UPDATE 2025-11-24:** Original scope (8 command handlers, 16h) reduced to 4h per **ADR-018:
> Command Handlers Minimalism**. Focus on production-blocking bug fixes and error handling, not new
> handlers.

As a developer building adaptive workflows, I want robust command infrastructure with proper error
handling, So that the existing 4 core commands operate reliably in production.

**Acceptance Criteria:**

1. Fix BUG-001: Race condition in CommandQueue.processCommands()
   - Async/await properly handles Promise resolution
   - No commands lost during parallel processing
   - Integration tests verify fix

2. Improve command registry error handling:
   - Centralized command dispatch with Map registry
   - Try/catch wrappers around all handlers
   - Error events emitted for observability
   - Unknown commands logged as warnings (not errors)

3. Document Replan-First Architecture (ADR-018):
   - Update story with ADR-018 rationale
   - Add note to spike (over-scoping correction)
   - Update engineering backlog with deferred handlers

**Deferred Handlers** (See ADR-018 + engineering-backlog.md):

- ‚ùå `inject_tasks` - Redundant with `replan_dag`
- ‚ùå `skip_layer` - Safe-to-fail branches cover this
- ‚ùå `modify_args` - No proven HIL correction workflow yet
- ‚ùå `checkpoint_response` - Composition of existing handlers sufficient

**Prerequisites:** Story 2.5-3 (AIL/HIL integration)

**Related:** Engineering Backlog (BUG-001: Race condition in processCommands() should be fixed as
part of this story)

---

## Epic 3: Agent Code Execution & Local Processing

**Status:** ‚úÖ DONE (2025-11-24, 7/7 active stories, 3.3 deprecated) **Retrospective:** Completed

**Expanded Goal (2-3 sentences):**

Impl√©menter un environnement d'ex√©cution s√©curis√© permettant aux agents d'√©crire et d'ex√©cuter du
code TypeScript localement, traitant les donn√©es volumineuses avant injection dans le contexte LLM.
Ce troisi√®me epic ajoute une couche de processing local compl√©mentaire au vector search (Epic 1) et
au DAG execution (Epic 2), permettant de r√©duire davantage la consommation de contexte (de <5% √†
<1%) pour les cas d'usage avec large datasets, tout en prot√©geant les donn√©es sensibles via
tokenisation automatique des PII.

**Value Delivery:**

√Ä la fin de cet epic, un d√©veloppeur peut ex√©cuter des workflows qui traitent localement des
datasets volumineux (ex: 1000 commits GitHub), filtrent et agr√®gent les donn√©es dans un sandbox
s√©curis√©, et retournent seulement le r√©sum√© pertinent (<1KB) au lieu des donn√©es brutes (>1MB),
r√©cup√©rant 99%+ de contexte additionnel et prot√©geant automatiquement les donn√©es sensibles.

**Estimation:** 8 stories (3.1 √† 3.8)

**Design Philosophy:**

Inspir√© par l'approche Anthropic de code execution, Epic 3 combine le meilleur des deux mondes :
vector search (Epic 1) pour d√©couvrir les tools pertinents, puis code execution pour traiter les
r√©sultats localement. L'agent √©crit du code au lieu d'appeler directement les tools, permettant
filtrage, agr√©gation, et transformation avant que les donn√©es n'atteignent le contexte LLM.

**Safe-to-Fail Branches Pattern (Story 3.5):**

Une propri√©t√© architecturale critique qui √©merge d√®s que le sandbox est int√©gr√© au DAG (Story 3.5) :
les t√¢ches sandbox peuvent √©chouer sans compromettre l'ensemble du workflow, car elles s'ex√©cutent
dans un environnement isol√©.

Le combo **Speculative Execution (Epic 2) + Safe-to-Fail Branches (Epic 3)** transforme le DAG
executor en syst√®me de **speculative resilience**.

---

### Story Breakdown - Epic 3

**Story 3.1: Deno Sandbox Executor Foundation** ‚úÖ

**Story 3.2: MCP Tools Injection into Code Context** ‚úÖ

**Story 3.3: Local Data Processing Pipeline** ‚ùå DEPRECATED

> Architectural issue: breaks AIL, redundant with 3.4 DAG + code_execution

**Story 3.4: `pml:execute_code` MCP Tool** ‚úÖ

**Story 3.5: Safe-to-Fail Branches & Resilient Workflows** ‚úÖ

**Story 3.6: PII Detection & Tokenization** ‚úÖ

**Story 3.7: Code Execution Caching & Optimization** ‚úÖ

**Story 3.8: End-to-End Code Execution Tests & Documentation** ‚úÖ

**Story 3.9: Sandbox Security Hardening** ‚úÖ

---

## Epic 3.5: Speculative Execution with Sandbox Isolation

**Status:** ‚úÖ DONE **Retrospective:** Optional

**Expanded Goal (2-3 sentences):**

Impl√©menter speculation WITH sandbox pour THE feature diff√©renciateur - 0ms perceived latency avec
s√©curit√© garantie. Utiliser GraphRAG community detection et confidence scoring pour pr√©dire les
prochaines actions et ex√©cuter sp√©culativement dans sandbox isol√©, permettant rollback automatique
si pr√©diction incorrecte.

**Value Delivery:**

√Ä la fin de cet epic, Casys PML peut pr√©dire avec 70%+ de pr√©cision les prochaines actions d'un
workflow, les ex√©cuter sp√©culativement dans sandbox isol√© pendant que l'agent r√©fl√©chit, et fournir
r√©sultats instantan√©s (0ms perceived latency) quand l'agent demande finalement l'op√©ration.

---

### Story Breakdown - Epic 3.5

**Story 3.5-1: DAG Suggester & Speculative Execution** ‚úÖ

> Code Review APPROVED 2025-11-26, 14 unit tests passing

**Story 3.5-2: Confidence-Based Speculation & Rollback** ‚úÖ

> Code Review APPROVED 2025-11-28, 46 tests passing, production-ready

---

## Epic 4: Episodic Memory & Adaptive Learning (ADR-008)

**Status:** ‚úÖ DONE (2025-12-01, all 6 stories) **Retrospective:** Completed

**Expanded Goal (2-3 sentences):**

√âtendre Loop 3 (Meta-Learning) avec m√©moire √©pisodique pour persistence des contextes d'ex√©cution et
apprentissage adaptatif des seuils de confiance via algorithme Sliding Window + FP/FN detection.
Transformer Casys PML en syst√®me auto-am√©liorant qui apprend continuellement de ses ex√©cutions.

**Value Delivery:**

√Ä la fin de cet epic, Casys PML persiste son apprentissage entre sessions (thresholds ne sont plus
perdus au red√©marrage), utilise les √©pisodes historiques pour am√©liorer pr√©dictions (context-aware),
et ajuste automatiquement les thresholds de confiance pour maintenir 85%+ de success rate.

---

### Story Breakdown - Epic 4

**Story 4.1a: Schema PGlite** ‚úÖ

> Migration 007 created

**Story 4.1b: EpisodicMemoryStore Class** ‚úÖ

> 280 LOC, 9 tests

**Story 4.1c: Threshold Persistence** ‚úÖ

> Extended adaptive-threshold.ts (+100 LOC)

**Story 4.1d: ControlledExecutor Integration** ‚úÖ

> Code review APPROVED 2025-11-26

**Story 4.1e: DAGSuggester Context Boost** ‚úÖ

> Code Review APPROVED 2025-12-01, 6 tests passing

**Story 4.2: Adaptive Threshold Learning (Sliding Window + FP/FN Detection)** ‚úÖ

> Implemented 2025-11-05 during Epic 1

---

## Epic 5: Intelligent Tool Discovery & Graph-Based Recommendations

**Status:** ‚úÖ DONE (2025-11-27, both stories) **Retrospective:** Optional

### Vision

Am√©liorer la d√©couverte d'outils en combinant recherche s√©mantique et recommandations bas√©es sur les
patterns d'usage r√©els. Le probl√®me initial: `execute_workflow` utilisait PageRank pour la recherche
d'un seul outil, ce qui n'a pas de sens (PageRank mesure l'importance globale, pas la pertinence √†
une requ√™te).

### Technical Approach

**Hybrid Search Pipeline (style Netflix):**

1. **Candidate Generation** - Recherche s√©mantique (vector embeddings)
2. **Re-ranking** - Graph-based boost (Adamic-Adar, neighbors)
3. **Final Filtering** - Top-K results

**Algorithmes Graphology:**

- `Adamic-Adar` - Similarit√© bas√©e sur voisins communs rares
- `getNeighbors(in/out/both)` - Outils souvent utilis√©s avant/apr√®s
- `computeGraphRelatedness()` - Score hybride avec contexte

**Alpha Adaptatif:**

- `Œ± = 1.0` si 0 edges (pure semantic)
- `Œ± = 0.8` si < 10 edges
- `Œ± = 0.6` si > 50 edges (balanced)

---

### Story Breakdown - Epic 5

**Story 5.1: search_tools - Semantic + Graph Hybrid Search** ‚úÖ

**Story 5.2: Workflow Templates & Graph Bootstrap** ‚úÖ

> Code Review APPROVED 2025-11-27, 52 tests passing

---

## Epic 6: Real-time Graph Monitoring & Observability

**Status:** üîÑ IN-PROGRESS (5/6 stories done, 6-6 added 2025-12-29) **Retrospective:** Optional

### Vision

Fournir une visibilit√© compl√®te sur l'√©tat du graphe de d√©pendances en temps r√©el via un dashboard
interactif. Les d√©veloppeurs et power users pourront observer comment le graphe apprend et √©volue,
diagnostiquer les probl√®mes de recommandations, et comprendre quels outils sont r√©ellement utilis√©s
ensemble dans leurs workflows.

### Value Delivery

√Ä la fin de cet epic, un d√©veloppeur peut ouvrir le dashboard Casys PML et voir en direct :

- Le graphe complet avec nodes (tools) et edges (d√©pendances)
- Les √©v√©nements en temps r√©el (edge cr√©√©, workflow ex√©cut√©)
- Les m√©triques live (edge count, density, alpha adaptatif)
- Les outils les plus utilis√©s (PageRank top 10)
- Les communities d√©tect√©es par Louvain
- Les chemins de d√©pendances entre outils

### Technical Approach

**Architecture:**

- **Backend**: SSE endpoint `/events/stream` pour √©v√©nements temps r√©el
- **Frontend**: Page HTML statique avec D3.js/Cytoscape.js pour graph viz
- **Data Flow**: GraphRAGEngine ‚Üí EventEmitter ‚Üí SSE ‚Üí Browser
- **Performance**: Graph rendering <500ms pour 200 nodes

---

### Story Breakdown - Epic 6

**Story 6.1: Real-time Events Stream (SSE)** ‚úÖ

> Code Review APPROVED 2025-12-01, 12 tests passing, Quality Score: 100/100

**Story 6.2: Interactive Graph Visualization Dashboard** ‚úÖ

> Code Review APPROVED 2025-12-02, 7 tests passing, Quality Score: 95/100

**Story 6.3: Live Metrics & Analytics Panel** ‚úÖ

> Code Review APPROVED 2025-12-02, 23 tests passing, Quality Score: 95/100

**Story 6.4: Graph Explorer & Search Interface** ‚úÖ

> Code Review APPROVED 2025-12-04, 11 tests passing, all 11 ACs validated

**Story 6.5: EventBus with BroadcastChannel** üìã BACKLOG

> ADR-036 - Added 2025-12-08, requires Story 7.3b completion

As a dashboard user, I want all system events (tools, DAG, graph, capabilities) streamed in
real-time via a unified EventBus, So that I can monitor execution live without polling.

**Key Deliverables:**

- Unified EventBus singleton using BroadcastChannel
- Migration of all event sources (tools, DAG, graph, capabilities)
- SSE Handler refactored to consume via EventBus
- Metrics collector subscribing to events

**Prerequisites:** Story 7.3b (introduces BroadcastChannel for capability traces)

**Story file:** `docs/sprint-artifacts/6-5-eventbus-broadcast-channel.md`

---

**Story 6.6: Admin Analytics Dashboard (Cloud-Only)** üîÑ IN-PROGRESS

As a platform admin, I want a technical analytics dashboard showing user activity, system health, and error rates, So that I can monitor platform usage and diagnose issues.

**Context:**

Ce dashboard est destin√© aux admins/op√©rateurs, pas aux utilisateurs normaux. Il expose des m√©triques techniques pour surveiller la sant√© de la plateforme en mode cloud.

**‚ö†Ô∏è Cloud-Only:** Ce code est exclu du sync public via `src/cloud/` et `src/web/`.

**Acceptance Criteria:**

1. **User Activity Metrics** (`/api/admin/analytics`):
   - Active users (daily/weekly/monthly)
   - New registrations over time
   - User retention (returning users)
   - Top users by usage (anonymized or by consent)

2. **System Usage Metrics**:
   - Total MCP calls (per day/week)
   - Capability executions count
   - DAG executions count
   - Average calls per user

3. **Error & Health Metrics**:
   - Error rate (% of failed executions)
   - Errors by type (timeout, permission, runtime)
   - Latency percentiles (p50, p95, p99)
   - Rate limit hits count

4. **Resource Metrics**:
   - SHGAT training frequency
   - PER buffer size
   - Graph node/edge counts
   - DB storage usage

5. **Admin-only Access**:
   - Route protected by admin role check
   - Returns 403 for non-admin users
   - Local mode: accessible by default (single user = admin)

6. **Dashboard UI** (`/dashboard/admin`):
   - Time range selector (24h, 7d, 30d)
   - Charts for trends (usage over time, error rates)
   - Tables for top users, frequent errors
   - Real-time updates via SSE (optional)

**File Structure (Cloud-Only):**

```
src/cloud/admin/
‚îú‚îÄ‚îÄ mod.ts                 # Export public API
‚îú‚îÄ‚îÄ analytics-service.ts   # Service layer
‚îú‚îÄ‚îÄ analytics-queries.ts   # SQL aggregations
‚îî‚îÄ‚îÄ types.ts              # Analytics types

src/web/routes/dashboard/
‚îî‚îÄ‚îÄ admin.tsx             # Fresh UI (already excluded via src/web/)
```

**Technical Notes:**

- Data aggregation from: `execution_trace`, `dag_executions`, `users`
- Consider materialized views for expensive queries
- Cache dashboard data (1-5 min TTL)
- Admin role: `users.role = 'admin'` or env-based allowlist

**Prerequisites:** Story 9.5 (user_id FK), Story 6.1 (SSE events)

**Estimation:** 2-3 jours
