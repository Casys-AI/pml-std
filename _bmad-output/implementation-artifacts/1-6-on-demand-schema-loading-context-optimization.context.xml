<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>1.6</storyId>
    <title>On-Demand Schema Loading & Context Optimization</title>
    <status>drafted</status>
    <generatedAt>2025-11-04</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/story-1.6.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Claude Code user</asA>
    <iWant>Casys Intelligence to load only relevant tool schemas based on my query</iWant>
    <soThat>my context window is not saturated by unused tool schemas</soThat>
    <tasks>
      <task id="1.6.1">Implement ContextOptimizer class with vector search integration</task>
      <task id="1.6.2">Create schema loading workflow (query → search → load)</task>
      <task id="1.6.3">Implement context usage measurement and logging</task>
      <task id="1.6.4">Add before/after comparison metrics display</task>
      <task id="1.6.5">Implement schema caching with LRU eviction</task>
      <task id="1.6.6">Verify P95 latency &lt;200ms requirement</task>
      <task id="1.6.7">Add metrics tracking to database</task>
      <task id="1.6.8">Write unit and integration tests</task>
      <task id="1.6.9">Add documentation with usage examples</task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="AC1">Integration semantic search avec schema loading</criterion>
    <criterion id="AC2"
    >Workflow: query → vector search → retrieve top-k tools → load schemas</criterion>
    <criterion id="AC3"
    >Schemas retournés uniquement pour matched tools (pas all-at-once)</criterion>
    <criterion id="AC4">Context usage measurement et logging (&lt;5% target)</criterion>
    <criterion id="AC5">Comparison metric affiché: before (30-50%) vs after (&lt;5%)</criterion>
    <criterion id="AC6">Cache hit pour frequently used tools (évite reloading)</criterion>
    <criterion id="AC7">Performance: Total query-to-schema latency &lt;200ms P95</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/PRD.md</path>
        <title>Product Requirements Document</title>
        <section>Context Optimization Requirements</section>
        <snippet
        >FR001-FR004: System must generate embeddings, perform semantic search (k=3-10), load schemas on-demand, and maintain context usage &lt;5%</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Decision Architecture</title>
        <section>Project Structure - Vector Search</section>
        <snippet
        >src/vector/ contains embeddings.ts (Story 1.4 BGE model), search.ts (Story 1.5 semantic search), and index.ts (HNSW index). PGlite with pgvector provides HNSW index for &lt;100ms P95 queries</snippet>
      </doc>
      <doc>
        <path>docs/stories/story-1.3.md</path>
        <title>Story 1.3: MCP Server Discovery &amp; Schema Extraction</title>
        <section>Prerequisite</section>
        <snippet
        >Completed. SchemaExtractor discovers MCP servers via stdio/SSE, extracts tool schemas via list_tools, stores in tool_schema table. Supports 15+ servers concurrently.</snippet>
      </doc>
      <doc>
        <path>docs/stories/story-1.4.md</path>
        <title>Story 1.4: Embeddings Generation with BGE-Large-EN-v1.5</title>
        <section>Prerequisite</section>
        <snippet
        >Completed. BGE-Large-EN-v1.5 generates 1024-dim embeddings for tool schemas. Embeddings cached, &lt;2 min for 200 tools. Used @xenova/transformers.</snippet>
      </doc>
      <doc>
        <path>docs/stories/story-1.5.md</path>
        <title>Story 1.5: Semantic Vector Search Implementation</title>
        <section>Prerequisite (in review)</section>
        <snippet
        >VectorSearch.searchTools(query, topK, minScore) API implemented with cosine similarity. P95 &lt;100ms target for 1000+ vectors using HNSW index.</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/vector/search.ts</path>
        <kind>service</kind>
        <symbol>VectorSearch</symbol>
        <lines>33-120</lines>
        <reason
        >Core class for semantic search. Contains searchTools() API that must be integrated with schema loading for on-demand context optimization.</reason>
      </artifact>
      <artifact>
        <path>src/vector/embeddings.ts</path>
        <kind>service</kind>
        <symbol>EmbeddingModel</symbol>
        <lines>1-100</lines>
        <reason
        >BGE-Large-EN-v1.5 model for generating query embeddings. Required for searchTools() to work.</reason>
      </artifact>
      <artifact>
        <path>src/mcp/schema-extractor.ts</path>
        <kind>service</kind>
        <symbol>SchemaExtractor</symbol>
        <lines>31-80</lines>
        <reason
        >Extracts and stores MCP tool schemas. Will be used to load schemas on-demand after vector search identifies relevant tools.</reason>
      </artifact>
      <artifact>
        <path>src/db/migrations/001_initial.sql</path>
        <kind>schema</kind>
        <symbol>tool_schema, tool_embedding</symbol>
        <lines>6-34</lines>
        <reason
        >Database schema with tool_schema (tool definitions) and tool_embedding (vectors) tables. New metrics table may be needed for AC4.</reason>
      </artifact>
      <artifact>
        <path>src/mcp/types.ts</path>
        <kind>types</kind>
        <symbol>MCPTool, MCPServer</symbol>
        <lines>1-50</lines>
        <reason
        >Type definitions for MCP tools and servers. Used throughout schema loading workflow.</reason>
      </artifact>
    </code>
    <dependencies>
      <node>
        <package name="@electric-sql/pglite" version="0.3.11"
        >Embedded PostgreSQL WASM with pgvector support</package>
        <package name="@xenova/transformers" version="2.17.2"
        >BGE-Large-EN-v1.5 model for embeddings</package>
        <package name="@std/log">Deno standard logging</package>
        <package name="@std/yaml">YAML config parsing</package>
        <package name="@std/assert" version="1.0.11">Testing assertions</package>
      </node>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint
    >Must reuse existing VectorSearch class from src/vector/search.ts for semantic search</constraint>
    <constraint
    >Must use EmbeddingModel from src/vector/embeddings.ts for query embeddings</constraint>
    <constraint
    >New ContextOptimizer class should be created in src/context/ directory following project structure</constraint>
    <constraint>Must achieve &lt;5% context usage target (FR004 from PRD)</constraint>
    <constraint>Must maintain P95 latency &lt;200ms for full query-to-schema workflow</constraint>
    <constraint
    >Implement LRU cache with configurable MAX_CACHE_SIZE (suggested 50 per story notes)</constraint>
    <constraint>Use structured logging with @std/log (info, warn, error levels)</constraint>
    <constraint
    >Follow Deno conventions: TypeScript strict mode, ES modules, no implicit any</constraint>
    <constraint
    >New metrics table must be added via migration if tracking context usage to database</constraint>
    <constraint
    >Token estimation should use rough heuristic (500 tokens per schema as per story notes)</constraint>
  </constraints>
  <interfaces>
    <interface>
      <name>VectorSearch.searchTools</name>
      <kind>function</kind>
      <signature
      >async searchTools(query: string, topK: number = 5, minScore: number = 0.7): Promise&lt;SearchResult[]&gt;</signature>
      <path>src/vector/search.ts</path>
    </interface>
    <interface>
      <name>SearchResult</name>
      <kind>interface</kind>
      <signature
      >{ toolId: string; serverId: string; toolName: string; score: number; schema: MCPTool }</signature>
      <path>src/vector/search.ts</path>
    </interface>
    <interface>
      <name>EmbeddingModel.generateEmbedding</name>
      <kind>function</kind>
      <signature>async generateEmbedding(text: string): Promise&lt;number[]&gt;</signature>
      <path>src/vector/embeddings.ts</path>
    </interface>
    <interface>
      <name>PGliteClient</name>
      <kind>class</kind>
      <signature>Database client for executing SQL queries and transactions</signature>
      <path>src/db/client.ts</path>
    </interface>
  </interfaces>
  <tests>
    <standards
    >
      Use Deno.test framework with @std/assert for assertions (assertEquals, assertExists, assert).
      Create in-memory test databases using PGliteClient("memory://") for isolation.
      Structure tests by AC (one test per acceptance criterion).
      Use performance.now() for latency measurements.
      Run tests with: deno test --allow-all.
      Run benchmarks with: deno bench --allow-all for P95 latency verification.
      Test files follow pattern: tests/unit/{module}/{feature}_test.ts
    </standards>
    <locations
    >
      tests/unit/context/ - New directory for ContextOptimizer tests
      tests/integration/ - For end-to-end workflow tests
      tests/benchmark/ - For P95 latency verification
    </locations>
    <ideas>
      <idea ac="AC1"
      >Test that ContextOptimizer integrates VectorSearch correctly: query → searchTools → returns relevant schemas</idea>
      <idea ac="AC2"
      >Test full workflow: getRelevantSchemas(userQuery, topK) performs vector search and returns only top-k schemas</idea>
      <idea ac="AC3"
      >Test that schemas are loaded on-demand (not all-at-once): verify only topK schemas returned, not full catalog</idea>
      <idea ac="AC4"
      >Test context usage measurement: verify logContextUsage() calculates percentage correctly and stays &lt;5% for typical scenarios</idea>
      <idea ac="AC5"
      >Test before/after comparison display: verify showContextComparison() outputs meaningful metrics (25% → 1.25% example)</idea>
      <idea ac="AC6"
      >Test LRU cache: verify SchemaCache.get/set work correctly, LRU eviction happens when MAX_CACHE_SIZE reached, cache hits counted</idea>
      <idea ac="AC7"
      >Benchmark P95 latency: measure full query-to-schema latency across 100+ queries, verify P95 &lt;200ms</idea>
      <idea
      >Integration test: End-to-end scenario with real embeddings, vector search, and schema loading</idea>
      <idea
      >Edge case: Empty query handling, invalid topK values, zero results from vector search</idea>
    </ideas>
  </tests>
</story-context>
